In this first part we will make our video trans-coding service and pipeline on AWS.

1. First go to AWS and log in,where we will start off by going to the S3 service which can be found in the list of storage services.
   
   * Make sure to create this and all other resources and services in (US-East-1) N. Virginia .
      
    ** I'm not sure yet as why we need to create all of our resources in N. Virginia, but by the end we will figure out what the reason behind this is.
    
       I remember when I created my first Alexa skill (to poke fun of my good friend Jon and cofounder of Mined Minds) that we had to keep everything needed in the N. Virginia region , My Jon says Alexa skill executes a lambda (fun fact: an Alexa skill is essentially just a Lambda, which we will learn to create later on in this project), and any lambda created for Alexa skills need to be located within the same region as the services that control her core logic which is N.Virginia (us-east-1).
       
       AWS services also tend to communicate more efficiently between each other when all resources & services are within the same region with some even requiring it, for example when creating an API using Amazon's API Gateway the API endpoint and the lambda need to be located within the same region as each other in order to be seen by one another.
       
       If you can I would recommend keeping any services or resources that may need to interact with one another in the same region as each other, however any and all IAM (Identity and Access Management) roles or polices need to be within the same region as the AWS service(s) you are using regardless. 
            

-------------------------------------------------------------------------------------------------------------

2. In this step we will create 2 buckets 
  
  A. We start by clicking the "Create bucket" button .
  B. Give your bucket a name, the first bucket we make is going to hold our videos after being uploaded and before the are trans-coded so lets give it a memorable name such as "youruniqueandcoolname-video-upload" so we won't forget what a bucket may contain later on.
  C. Pick a region for your bucket, and remember that we want to choose N. Virginia (us-east-1) for our region.
  D. Click create to create your bucket.
  E. Repeat the process for the bucket which will hold our videos that have been uploaded and through tans-coding process. Give the bucket a unique name you will remember, choose the region and hit "Create bucket" once again.
  

--------------------------------------------------------------------------------------------------------------

3. Having created our buckets we need to create the correct policy needed by our bucket which will allow it to be publicly accessed

  A. Click on the name of the bucket which holds our post tans-coded videos (the second bucket we made)
  B. Click the premissons tab, click on bucket policy in order to give our bucket the premssions needed to access and view the video after having been trans-coded. You can use the below example which can also be found in s3bucket-required-rolesandpolicies.txt . 
    
    -Example:
      
      {
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "AddPerm",
			"Effect": "Allow",
			"Principal": "*",
			"Action": "s3:GetObject",
			"Resource": "arn:aws:s3:::<YOUR-BUCKET-NAME>/*"
		}
	]
}
     
   * Make sure to remember to rename <YOUR-BUCKET-NAME> to the name you gave to your trans-coded videos bucket *
   
   
  
   
  ----------------------------------------------------------------------------------------------------------------------
 
 
 4. Next we	need to	create an IAM role that will allow access	to S3 and to the Elastic Transcoder from a future lambda which we will create soon (Hooray..... lambdas again !!!!!, for real though they can be pretty cool)
 
   A. Start off by going to the IAM service which can be found by clicking the services tab on the menu bar at the top of the page *    * If you are having trouble locating it look under the Security, Identity & Compliance section.
   B. Choose AWS Service (Should be pre-selected) and choose Lambda and click the "Next:Premissons" button.
   C. Click the checkboxes next to the following policies.
      C1. AWSLambdaExecute	
        C1-Policy.
          This should be created for you automatically by amazon, but if it doesn't for some reason or if you just want to know what it looks like in JSON format which all roles and policies are saved in. You can also find this located in the s3bucket-required-rolesandpolicies.txt file.
          
          {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": [
                  "logs:*"
              ],
              "Resource": "arn:aws:logs:*:*:*"
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:GetObject",
                  "s3:PutObject"
              ],
              "Resource": "arn:aws:s3:::*"
          }
      ]
  }
      C2. AmazonElasticTranscoderJobsSubmitter
        C2-Policy.
          This also should be created for you automatically by amazon, but if it did't for some reason or if you just wanted to see what it looked like in JSON format. You can also find this located in the s3bucket-required-rolesandpolicies.txt file.
          
          {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "elastictranscoder:Read*",
                "elastictranscoder:List*",
                "elastictranscoder:*Job",
                "elastictranscoder:*Preset",
                "s3:List*",
                "iam:List*",
                "sns:List*"
            ],
            "Effect": "Allow",
            "Resource": "*"
        }
    ]
}
         
   D. Click "Next:Review" to give it a name, remember to give it a name that is clear what it is doing or the services being accessed (such as lambda-s3-execution-role) Give it a brief description and review that everything is correct before clicking on the "create role" button and making your new role, if it looks good click "Create Role"
   E. You will be taken to the roles section where you can review the role you just made by selecting the role name you chose, here you can review the policies you added to this or any role. You can also get to the role section by clicking the roles tab loated on the left side of the page.
  
  
  ----------------------------------------------------------------------------------------------------------------------------------
  
   

5. Next we will need to setup an Elastic Transcoder pipeline, using Amazon Elastic Transcoder lets you convert any digital media stored in an Amazon S3 Bucket into the audio and video codecs and the containers required by various consumer playback devices.

  A. Click the Services tab at the top of the page and underneath of the Application Services section choose Elastic Transcoder.
  B. Click on "Create a New Pipeline" to begin creating the pipeline that will take a uploaded video and transcode in to a usable format for the wide array of people and devices out there.
  C. Give your pipeline a name and specify the bucket where the inital video is uploaded to (yourcoolname-upload-bucket).
  D. Leave the IAM role as it is, which should be create console default role that is created automatically by Elastic Transcoder
  E. Under Configuration for Amazon S3 Bucket for Transcoded Files and Playlists choose the bucket (yourcoolname-transcoded-videos) which will contain the transcoded videos once going through the pipeline.
  F. Under Storage type choose Standard.
  G. Next we will want to choose a bucket to store thumbnails even though we are not yet using them at this time, Choose the same settings as we did for the transcoded-videos bucket.
  H. Next we can add notifactions or encryption but for now lets leave the settings as they are.
  I. Click "Create Pipeline" to create our pipeline using the settings specified. 
  J.Take a look at the summary for the pipeline you just created, and WRITE DOWN THE PIPELINE ID you will need it again and soon.


------------------------------------------------------------------------------------------------------------------------------------

6. We next have the pleasure of creating our first lambda function (YEAAAAA!!! Lambdas are cool)
  
  A. Proceed to Lambda service located under the Compute section in the list of services.
  B. Click on Author from scratch
  C. Give your lambda a name, remember to be descriptive and as always WRITE IT DOWN somewhere and store in a secure place.
  D. Under the Role section choose "eisting role"
  E. Under the existing role section choose the role that we created earlier, which we named lambda-s3-execution-role
  F. Click on "create function" to create our first lambda for our project
  G. Once the Lambda is created leave the settings as the are for now or if you choose to a pick a different node runtime, however for now leave code entry type the way it is for now and leave the Handler name the default (index.handler) unless you know more about it and if so cool feel free to change it to whatever you like but make sure to remember to change it through out the code too)
  H. Click on the configuration tab and Scroll down under Basic settings change the timeout of the Lambda to 0 min 30 sec, this is to ensure if our video transcoding takes longer then it should have it doesn't kep running our lambda wasting valuable server uptime and resources saving you money.
  I. Finally we can start to deploy the actual lambda code to Amazon. For more detail, tips and other cool tidbits on what is happening within our lambda function checkout the index.js file.
  J. We first start by opening up the terminal and navagating to the directory where lambda code file are stored and running the command "npm install" to install any needed node modules and dependicies.
  K. Next we can run "npm	run	predeploy" which runs the predeploy script located in the package.json file zipping up the required files for our lambda while ignoring files or folders we dont want or need to use.
  L. Remember to add node_modules to your gitignore file as the node modules folder tends to get big and there is no reason to deploy it to github when we can just run a npm install when needed for any modules and dependecies needed.
  M. Back in Amazon go back to our Lambda function we made and if you didnt earlier chnage the node rune time to 4.3
  N. Looking under the code entry type choose upload a zip file and choose the zip file that was created with "npm run predeploy"
  O. Lets finally add in our enviromental variables buy clicking on the Environment variables tab.
    Oa. ELASTIC_TRANSCODER_REGION = us-east-1 (in a key,value pair )
    Ob. ELASTIC_TRANSCODER_PIPELINE_ID = PIPELINEIDGOESHERE (key,value pair as well) We got this value earier when we first made our pipeline and you remembered to write down right?"
    Oc. ARN = YOURLAMBDAARN, Add in our env for the ARN located inside of our json file hope it works (I'll update this shortly when I find out.)
  P. Click on the "add triggers" tab and click on the square dashed box to add a trigger.
  Q. Scroll the list of triggers and pick S3.
      Q1. Pick the bucket that the trigger will watch (youruniqueandcoolname-video-upload) for a specified action
      Q2. The action we want to watch for is "Object Created (ALL)"
      Q3. Press "Submit" to link a trigger to your lambda.
  R. Having done evrything so far lets upload a video to test out, we will do this one manually through the S3 UI and navigate to the "youruniqueandcoolname-video-upload" bucket.
  S. Click on the upload button then on the  "Add files" button to choose the file you want upon choosing the file you want click the "Upload files" button to start the uploading process to your S3 bucket.
  T. You should have the uploaded file in your "youruniqueandcoolname-video-upload" bucket, and after giving it a couple of minutes we can check our "youruniqueandcoolname-transcoded-videos" bucket to see the 3 new files that should have been created by the process.
  U. Having checked the "youruniqueandcoolname-transcoded-videos" we can see 3 files located within a folder with the name of the video we just uploaded containing videos with different bitrates and formats to pick from that will be shown based on users device.
 
 
 -----------------------------------------------------------------------------------------------------------------------------------
 
 8. Congradulations, You now have your very own serverless	video	transcoding	pipeline which is pretty awesome.
 
    So far we have used the following Amazon services, a S3 bucket (for storage of videos before and after transcoding), a Lambda that contains our code calling the Elastic Transcoder API to properly transcode our videos in to the correct formats, interacted with Amazon's Elastic Transcoder (that does the actual transcoding of a video after upload to  "youruniqueandcoolname-video-upload"'s bucket), a Lambda trigger that watches a S3 bucket for any upload, learned to create policies for our S3 bucket, made IAM roles and set policies upon that role and more.
        